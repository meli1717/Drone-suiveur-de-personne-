_Dans cette étape, il y a deux opérations qui vont être effectuées :
1. capturer la vidéo de la caméra
2. la comparer avec votre fichier .yml
_J'ai écrie le programme suivant dans le fichier ‘detector.py’

///////////////////////////////////////////////////////////////////////////

import cv2
import numpy as np

faceDetect=cv2.CascadeClassifier('/home/pi/opencv/data/haarcascades/haarcascade_frontalface_default.xml')
cam = cv2.VideoCapture(0)
rec=cv2.face.LBPHFaceRecognizer_create()
rec.read("reconnaissance/trainingData.yml")
id=0
font = cv2.FONT_HERSHEY_SIMPLEX
fontScale = 1
fontColor = (200, 200, 200)

while(True):
  ret, img = cam.read()
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  faces = faceDetect.detectMultiScale(gray, 1.3, 5)
  for (x,y,w,h) in faces:
    cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),2)
    id,conf=rec.predict(gray[y:y+h,x:x+w])
    if (id==3) :
      id="Meli"
    if (id==2) :
      id="Melinda"
    cv2.putText(img,str(id),(x,y+h),font,fontScale,fontColor);
  cv2.imshow("reconnaissance faciale",img)
  if(cv2.waitKey(1)==ord("q")):
    break;
cam.release()
cv2.destroyAllWindows()

//////////////////////////////////////////////////////////////////////////////////
